#!/usr/bin/env python3
"""
LLM API æµ‹è¯•æœåŠ¡å™¨
ä¸ºå‰ç«¯æä¾›LLM APIè°ƒç”¨æœåŠ¡
"""

import os
import sys
import asyncio
import json
from datetime import datetime
from pathlib import Path
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, '/Users/huizhang/PycharmProjects/S3DA2/SAFE-BMAD')

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv('/Users/huizhang/PycharmProjects/S3DA2/SAFE-BMAD/.env')

from fastapi import FastAPI, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

from core.llm.types import LLMConfig, LLMProvider, LLMMessage
from core.llm.manager import get_llm_manager
from core.llm.adapters import DeepSeekAdapter, GLMAdapter, OpenAIAdapter


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="LLM API æµ‹è¯•å¹³å°",
    description="æ”¯æŒ DeepSeekã€GLMã€OpenAI ç­‰å¤šä¸ªå¤§æ¨¡å‹æä¾›å•†",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
llm_manager = get_llm_manager()
active_connections = {}


class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚æ¨¡å‹"""
    provider: str
    message: str
    conversation_history: list = []
    temperature: float = 0.7
    max_tokens: int = 1000


class ChatResponse(BaseModel):
    """èŠå¤©å“åº”æ¨¡å‹"""
    success: bool
    content: str = None
    error: str = None
    metadata: dict = {}
    provider: str = None
    model: str = None
    response_time: float = None
    usage: dict = None


@app.get("/")
async def read_root():
    """è¿”å›å‰ç«¯é¡µé¢"""
    try:
        # è¯»å–HTMLæ–‡ä»¶
        html_path = Path("/Users/huizhang/PycharmProjects/S3DA2/SAFE-BMAD/web/llm_test_frontend.html")
        with open(html_path, 'r', encoding='utf-8') as f:
            html_content = f.read()

        return HTMLResponse(content=html_content)

    except FileNotFoundError:
        return JSONResponse(
            status_code=404,
            content={"error": "å‰ç«¯é¡µé¢æœªæ‰¾åˆ°"}
        )
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"æœåŠ¡å™¨é”™è¯¯: {str(e)}"}
        )


@app.post("/api/chat")
async def chat(request: ChatRequest):
    """å¤„ç†èŠå¤©è¯·æ±‚"""
    try:
        print(f"ğŸ” æ”¶åˆ°èŠå¤©è¯·æ±‚: {request.provider} - {request.message[:50]}...")

        # éªŒè¯æä¾›å•†
        if request.provider not in ["deepseek", "glm", "openai", "local"]:
            raise HTTPException(
                status_code=400,
                detail=f"ä¸æ”¯æŒçš„æä¾›å•†: {request.provider}"
            )

        # è·å–APIå¯†é’¥
        api_key = get_api_key(request.provider)
        if not api_key:
            raise HTTPException(
                status_code=400,
                detail=f"æœªæ‰¾åˆ° {request.provider} çš„APIå¯†é’¥"
            )

        # åˆ›å»ºæˆ–è·å–é€‚é…å™¨
        adapter_id = f"chat_{request.provider}"

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨é€‚é…å™¨
        if adapter_id not in active_connections:
            # åˆ›å»ºæ–°é€‚é…å™¨
            config = LLMConfig(
                provider=get_provider_enum(request.provider),
                model=get_default_model(request.provider),
                api_key=api_key,
                temperature=request.temperature,
                max_tokens=request.max_tokens
            )

            success = await llm_manager.create_adapter(
                adapter_id=adapter_id,
                config=config,
                is_default=False
            )

            if not success:
                raise HTTPException(
                    status_code=500,
                    detail=f"åˆ›å»º {request.provider} é€‚é…å™¨å¤±è´¥"
                )

            active_connections[adapter_id] = {
                "created_at": datetime.now(),
                "provider": request.provider,
                "model": get_default_model(request.provider)
            }

            print(f"âœ… åˆ›å»º {request.provider} é€‚é…å™¨æˆåŠŸ")

        # è½¬æ¢æ¶ˆæ¯æ ¼å¼
        messages = []

        # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
        messages.append(LLMMessage(role="system", content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”ã€‚"))

        # æ·»åŠ å†å²å¯¹è¯
        for hist_msg in request.conversation_history[-10:]:  # é™åˆ¶å†å²é•¿åº¦
            if hist_msg.get("role") and hist_msg.get("content"):
                messages.append(LLMMessage(
                    role=hist_msg["role"],
                    content=hist_msg["content"]
                ))

        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append(LLMMessage(role="user", content=request.message))

        # è°ƒç”¨LLM
        start_time = datetime.now()

        try:
            response = await llm_manager.chat_completion(
                adapter_id=adapter_id,
                messages=messages,
                max_tokens=request.max_tokens,
                temperature=request.temperature
            )

            end_time = datetime.now()
            response_time = (end_time - start_time).total_seconds()

            # æ›´æ–°è¿æ¥ä¿¡æ¯
            if adapter_id in active_connections:
                active_connections[adapter_id]["last_used"] = end_time
                active_connections[adapter_id]["request_count"] = active_connections[adapter_id].get("request_count", 0) + 1

            print(f"âœ… {request.provider} å“åº”æˆåŠŸ: {response.content[:100] if response.content else 'None'}...")

            return ChatResponse(
                success=True,
                content=response.content,
                provider=request.provider,
                model=get_default_model(request.provider),
                response_time=response_time,
                usage=response.usage,
                metadata={
                    "adapter_id": adapter_id,
                    "total_messages": len(messages),
                    "timestamp": end_time.isoformat()
                }
            )

        except Exception as llm_error:
            end_time = datetime.now()
            error_response = str(llm_error)
            response_time = (end_time - start_time).total_seconds()

            print(f"âŒ {request.provider} LLMè°ƒç”¨å¤±è´¥: {error_response}")

            return ChatResponse(
                success=False,
                error=error_response,
                provider=request.provider,
                response_time=response_time,
                metadata={
                    "error_type": type(llm_error).__name__,
                    "timestamp": end_time.isoformat()
                }
            )

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨é”™è¯¯: {str(e)}")
        return ChatResponse(
            success=False,
            error=f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {str(e)}",
            provider=request.provider,
            response_time=0.0,
            metadata={
                "error_type": "server_error",
                "timestamp": datetime.now().isoformat()
            }
        )


@app.get("/api/status")
async def get_status():
    """è·å–ç³»ç»ŸçŠ¶æ€"""
    return {
        "status": "running",
        "providers": {
            "deepseek": {
                "available": bool(os.getenv("DEEPSEEK_API_KEY")),
                "name": "DeepSeek",
                "models": ["deepseek-chat", "deepseek-coder"]
            },
            "glm": {
                "available": bool(os.getenv("GLM_API_KEY")),
                "name": "GLM (æ™ºè°±AI)",
                "models": ["glm-4", "glm-3-turbo", "glm-4v"]
            },
            "openai": {
                "available": bool(os.getenv("OPENAI_API_KEY")),
                "name": "OpenAI",
                "models": ["gpt-4", "gpt-3.5-turbo"]
            },
            "local": {
                "available": False,  # éœ€è¦æœ¬åœ°æœåŠ¡è¿è¡Œ
                "name": "Local Model",
                "models": ["local-chat", "local-coder"]
            }
        },
        "active_connections": len(active_connections),
        "connection_details": active_connections,
        "timestamp": datetime.now().isoformat()
    }


@app.get("/api/models/{provider}")
async def get_provider_models(provider: str):
    """è·å–æŒ‡å®šæä¾›å•†çš„æ¨¡å‹åˆ—è¡¨"""
    try:
        from core.llm.types import get_available_models, get_model_info, LLMProvider

        if provider not in ["deepseek", "glm", "openai", "local"]:
            raise HTTPException(status_code=404, detail="æä¾›å•†æœªæ‰¾åˆ°")

        provider_enum = get_provider_enum(provider)
        models = get_available_models(provider_enum)

        model_details = []
        for model in models:
            model_info = get_model_info(provider_enum, model)
            if model_info:
                model_details.append({
                    "name": model,
                    "description": model_info.get("description", ""),
                    "capabilities": [cap.value for cap in model_info.get("capabilities", [])],
                    "max_tokens": model_info.get("max_tokens", 0)
                })

        return {
            "provider": provider,
            "models": model_details
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {str(e)}")


# è¾…åŠ©å‡½æ•°
def get_api_key(provider: str) -> str:
    """è·å–APIå¯†é’¥"""
    env_mappings = {
        "deepseek": "DEEPSEEK_API_KEY",
        "glm": "GLM_API_KEY",
        "openai": "OPENAI_API_KEY",
        "local": "LOCAL_API_KEY"
    }

    env_key = env_mappings.get(provider)
    return os.getenv(env_key) if env_key else None


def get_provider_enum(provider: str) -> LLMProvider:
    """è·å–æä¾›å•†æšä¸¾"""
    provider_mappings = {
        "deepseek": LLMProvider.DEEPSEEK,
        "glm": LLMProvider.GLM,
        "openai": LLMProvider.OPENAI,
        "local": LLMProvider.LOCAL
    }
    return provider_mappings.get(provider)


def get_default_model(provider: str) -> str:
    """è·å–é»˜è®¤æ¨¡å‹"""
    default_models = {
        "deepseek": "deepseek-chat",
        "glm": "glm-4",
        "openai": "gpt-3.5-turbo",
        "local": "local-chat"
    }
    return default_models.get(provider, "deepseek-chat")


# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨LLM APIæµ‹è¯•æœåŠ¡å™¨...")
    print(f"ğŸ“ æœåŠ¡åœ°å€: http://localhost:8000")
    print(f"ğŸŒ å‰ç«¯é¡µé¢: http://localhost:8000")
    print(f"ğŸ“‹ APIæ–‡æ¡£: http://localhost:8000/docs")
    print(f"ğŸ”‘ APIçŠ¶æ€: http://localhost:8000/api/status")
    print("=" * 50)

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8080,
        reload=True,
        log_level="info"
    )